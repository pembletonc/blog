---
title: 'MAPS Crowdsourced Open Science Project: Data Imputation'
author: "Corey Pembleton"
date: '2019-05-20'
slug: maps-crowdsourced-open-science-project-data-imputation
categories: ["R", "hmisc", "mice"]
tags: ["data cleaning", "statistics", "tutorial"]
keywords: ["R", "Programming", "statistics"]
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

```

Missing data can cause a compromise in inferences made from clinicial trials, and the mechanism (or reason) why the data is missing in the first place implicates whether or not an analytic method can be used to correct that missingness, normally through imputation. There are risks to using analytic methods to impute missing values, such as creating a bias by imputing unavoidably missing data, and not understanding the mechanism by which data can be missing. There are three mechanisms which can cause missing data, it can be: missing completely at random (MCAR) missing at random (MAR), or and missing not at random (MNAR) [Jakobsen, Gluud, Wetterslev & Winkel (2017)](https://rdcu.be/bDwVn). 

I won't go into detail of each (check the cited paper), but will focus on the mechanism attributed by authors for missing data in this study. 

On this, the authors test whether the data is MNAR, and flag that:  

> Both anxiety and depression measured at age 7years were associated with non-response at age 18 (results not shown): individuals with evidence of anxiety and depression at age 7 were more likely to have missing outcome data at age 18, suggesting that the outcomes could be MNAR, or MAR conditional on anxiety and depression at age 7 (we acknowledge that this cannot be determined from the observed data). (pg 4, Khouja et. al (2019))

And thus, the authors chose to use multiple imputation (MI) to fill the data gaps present. After exploring the missing (and present) values of the synthetic ALSPAC dataset in a [previous post](https://data-break.netlify.com/2019/04/crowdsourced-open-science-project-data-exploration/), here I'm to explore different analytic approaches for replacing missing values, both as an exercise in learning new R packages as well as understanding what would be most appropriate for this analysis.

![Imputation isn't always a smooth process](/img/comic.jpg)

While I'm operating under the assumption that the method used by the authors is an acceptable method, I would like to understand data imputation in R (which I have never done before), and how different methods can impact future analysis differently. 


## Commonly Used Data Imputation Packages in R  
```{r, echo=FALSE, out.width="15%", fig.align='right', out.extra='style="float:right; padding:10px"'}

knitr::include_graphics("/img/comic2.gif")
```

After reviewing several blog posts e.g. [KDnuggets](Proportional odds model - suitable for ordered categorical variables with more than or equal to two levels ); [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/); and ___ , I've decided to review some of the mainstay popular packages used in R for data imputation.  

Each imputation package offers specific advantages, has underlying assumptions that must be understood (and respected)  



  

**MICE**

Methods:
- PMM (Predictive Mean Matching) - suitable for numeric variables 
- logreg(Logistic Regression) - suitable for categorical variables with 2 levels
- polyreg(Bayesian polytomous regression) - suitable for categorical variables with more than or equal to two levels 
- Proportional odds model - suitable for ordered categorical variables with more than or equal to two levels 

Assumptions:  
- assumes Missing at random (MAR) values


**Random Forests**

Assumptions:  
- 





