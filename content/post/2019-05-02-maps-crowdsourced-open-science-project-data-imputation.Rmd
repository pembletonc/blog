---
title: 'MAPS Crowdsourced Open Science Project: Data Imputation'
author: "Corey Pembleton"
date: '2019-06-05'
slug: maps-crowdsourced-open-science-project-data-imputation
categories: ["R"]
tags: ["data cleaning", "statistics", "tutorial"]
keywords: ["R", "Programming", "statistics"]
thumbnailImagePosition: "left"
thumbnailImage: "/img/cover_img.png"
coverImage: "/img/cover_img.png"
metaAlignment: "center"
coverMeta: "out"

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)

```

Missing data can cause a compromise in inferences made from clinicial trials, and the mechanism (or reason) why the data is missing in the first place implicates whether or not an analytic method can be used to correct that missingness at all. There are three mechanisms which can cause missing data: missing completely at random (MCAR) missing at random (MAR), or and missing not at random (MNAR) [Jakobsen, Gluud, Wetterslev & Winkel (2017)](https://rdcu.be/bDwVn).  

There are risks to using analytic methods to impute missing values, such as creating a bias by imputing unavoidably missing data, and not understanding the mechanism by which data can be missing. 


```{r, echo=FALSE, out.width="20%", out.height="20%", fig.align='centre', out.extra='style="padding:10px"', fig.cap="Imputation isn't always a smooth process"}

knitr::include_graphics("/img/comic.jpg")
```


I won't go into detail of each mechanism (check the cited paper above), but rather will focus on the mechanism attributed by authors for missing data in this study, which the authors note as possibly being MNAR or conditionally MAR, and selecting multiple imputation (MI) as the analytic method to replace missing values.  

> "Both anxiety and depression measured at age 7years were associated with non-response at age 18 (results not shown): individuals with evidence of anxiety and depression at age 7 were more likely to have missing outcome data at age 18, suggesting that the outcomes could be MNAR, or MAR conditional on anxiety and depression at age 7 (we acknowledge that this cannot be determined from the observed data)." (p. 4, Khouja et. al (2019))

## When is it acceptable to impute missing values?  
While I'm operating under the assumption that the method used by the authors is  acceptable, I would like to understand data imputation in R further (I have never worked with it before), and learn how different methods can impact future analysis differently.     

According to this flow chart created following a meta review of data imputation used in clinical trials, several of the conditions for using multiple imputation are questionable, although passable: that there is too large a proportion of the data missing (only 12.7% of cases are complete) and that an MNAR assumption is possible, although conditionally.


```{r, echo=FALSE, out.width="50%", fig.align='centre', out.extra='style="padding:10px"', fig.cap="Source: Jakobsen, Gluud, Wetterslev & Winkel (2017)"}

knitr::include_graphics("/img/imputation_flowchart.gif")
```


## How to impute missing values: commonly used imputation approaches and  packages in R  

### Methods uses in academic literature for imputation

- Complete case analysis: Complete case analysis is statistical analysis based on participates with a complete set of outcome data. Participants with any missing data are excluded from analysis.  

- Single Imputation: When using single imputation, missing values are replaced by a value defined by a certain rule, e.g. simple mean imputation, or last value carried forward.  

- Multiple Imputation: Multiple imputation has been shown to be a valid general method for handling missing data in randomised clinical trials, and this method is available for most types of data.  

```{r, echo=FALSE, out.width="15%", fig.align='right', out.extra='style="float:right; padding:10px"'}

knitr::include_graphics("/img/comic2.gif")
```


### R packages commonly used for imputation  
After reviewing several blog posts e.g. [KDnuggets](https://www.kdnuggets.com/2017/09/missing-data-imputation-using-r.html); [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/), I've decided to review one of the mainstaypackages used in R for data imputation, the ```mice``` package. In the future I would like to look closer at two other packages,```hmisc```, and ```missforest```.  


#### mice  

[Multivariate Imputation by Chained Equations](https://stefvanbuuren.name/mice/) package, developed by Stef van Buuren and Karin Groothuis-Oudshoorn.

```{r, cache=TRUE, error=FALSE, warning=FALSE, message=FALSE}

#will use a smaller version of the dataset with first 1,000 rows
orig_dat <- read_csv("https://raw.githubusercontent.com/pembletonc/OpenNorth_MAPS/master/maps-synthetic-data.csv") %>% rename(ID = X1)

vars_model1 <- c("dep_score", "text_wend", "text_week", "tv_wend", "tv_week",
          "comp_week", "comp_wend")

small_dat <- orig_dat %>% select(one_of("ID", vars_model1)) %>% head(1000)

#restructure the dataset renaming the categorical variables:

dat_1 <-  small_dat %>% mutate_at(vars(c(dep_score, text_wend, text_week, tv_wend, tv_week, comp_week, comp_wend)),
                    list(cat = ~case_when(. %in% c("Not at all") ~ 0,
                                          . %in% c("Less than 1 hour") ~ 1,
                                          . %in% c("1-2 hours") ~2,
                                          . %in% c("3 or more hours") ~3,
                                          TRUE ~NA_real_))) %>% 
  select(ID, dep_score, contains("cat"), -dep_score_cat) %>% 
  mutate_at(c("dep_score", "text_wend_cat", "text_week_cat",
              "tv_wend_cat", "tv_week_cat", "comp_week_cat", "comp_wend_cat"),
            funs(factor(.)))

```

```{r, error=FALSE, warning=FALSE, message=FALSE}
library(mice)
#following the vignette offered, the process looks like this:

ini <- mice(dat_1, maxit = 0)
meth <- ini$meth
meth

#set the method for factor variables with >= 2 levels with polyreg(Bayesian polytomous regression)
meth[3:8] <- "polyreg"
meth
pred <- ini$pred
pred[, "dep_score"] <- 0
pred[, "ID"] <- 0

```

Setting the pred of certain variables to 0 indicates they won't be used as predictor variables and creates a "predictor matrix". In this case, ```ID``` and```dep_score``` won't be used as a variable to predict the missing values (dep_score will be included in the second iteration):

```{r, echo=FALSE}
pred %>% kable() %>% kable_styling(full_width = F, position = "center")
```

## Convergence and inference pooling

In their [second vignette](https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html), the authors of the ```mice``` package provide a description of how to pool results and "avoid pitfalls researchers may fall into". The authors write that "In general, we would like the streams to intermingle and be free of any trends at the later iterations.", so I'm going to try to achieve this as well.


The mice function uses a Markov Chain Monte Carlo type of algorithm, and when reviewing the trace lines we can study convergence (here 5 iterations is used as the default minimum):

```{r, cache=TRUE}
#run the first imputation with the methods and predictor variables (currently none used)
imp1 <- mice(dat_1, meth = meth, pred = pred, print = FALSE)

plot(imp1, c("dep_score", "text_wend_cat","text_week_cat", "tv_wend_cat", "tv_week_cat"))

```

```{r}
#run second imputation, this time with depression_score as pred

pred <- ini$pred
pred[, "ID"] <- 0
pred
imp2 <- mice(dat_1, meth = meth, pred = pred, print = FALSE)

plot(imp2, c("dep_score", "text_wend_cat","text_week_cat", "tv_wend_cat", "tv_week_cat"))

```

A test for robustness can be increasing the amount of iterations on the imputation and comparing convergence, which is more convincing with more iterations (m = 20 here):

```{r, cache=TRUE}

imp3 <- mice.mids(imp2, maxit = 20, printFlag = FALSE)

plot(imp3, c("dep_score", "text_wend_cat","text_week_cat", "tv_wend_cat", "tv_week_cat"))

```

It doesn't look like there is significant convergence, however it does look like some variables have patterns emerging, which I plan to look into further.

Density plots comparing imputation against actual can help to measure approximate accuracy, here looking at a single variable:

```{r}
densityplot(imp2, ~text_week_cat)
```

As always, tabular results need to be examined as well:
```{r}
mice::complete(imp2, 1)[1:10,] %>%
  kableExtra::kable() %>%
  kable_styling(full_width = F, position = "center")
```

Looks pretty good! There are more tests for robustness but in this little example, it has been interesting to learn how to use imputation on datasets (especially with categorical variables).  

I see that imputation is in itself a field of study, and I look forward to learning more about it.  
I anticipate that once I begin comparing the full dataset to the imputed dataset, I'll gain a better understanding of how different imputation approaches can impact the data, and which will be best for the needs of this study.




