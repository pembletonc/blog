---
title: "OpenNorth Team Launches Contribution to University of Bristol MAPS crowdsourced analysis"
author: "Corey Pembleton"
date: '2019-04-27'
output: html_document
slug: opennorth-maps-crowdsourced-open-science-project
categories: [ "R", "dataexploration"]
tags: ["data cleaning", "statistics"]
thumbnailImagePosition: "left"
thumbnailImage: ""
draft: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(naniar)

```


My colleagues and I here at [OpenNorth](https://opennorth.ca) are contributing to a very interesting crowd-sourcing data analysis project out of the University of Bristol's Jean Golding Institute, *mapping the analytical paths of a crowdsourced data analysis*, or [MAPS](https://jean-golding-institute.github.io/maps/).


```{r, echo = FALSE, fig.align='center'}

blogdown::shortcode("tweet","1110577479507431425")

```

The MAPS study is largely a "[Many Analysts](https://journals.sagepub.com/doi/10.1177/2515245917747646)" study wherein many teams of analysts use the same data set to answer the same question seen in the tweet above. 

They've armed every analyst team with a data set, given full access to the original research piece answering the same question, detailed instructions, all accessible in a Open Science Framework page.

The MAPS project has several research questions is seeks to answer, the primary questions are concerned with replicability and comparability between the Many Analysts:

**Primary project questions:**  
1. How do teams’ results compare to the multiverse of results?  
2. How does the variability of teams’ results compare the variability of the multiverse of results?  
3. How does each team’s results compare to all the other teams’ results?  
4. What are effective ways of visually communicating the above comparisons?  

And the secondary questions are more focused on how the experiences of the Many Analysts impacts their results:


**Secondary project questions to be answered will be**:  
5. Does domain expertise affect the results of teams’ analyses?  
6. Does statistical expertise affect the results of teams’ analyses?  
7. Are teams’ subjective beliefs about the research question associated with the results of teams’ analyses?  
8. Does the anonymisation process affect the results of teams' analyses?  






### Synthetic dataset based upon real dataset used, the Avon Longitudinal Study of Parents and Children (ALSPAC)







Importantly, we also attempted to adjust for a range of other activities in
order to identify what other activities are sacrificed for screen time. Such measures include time spent outside, time spent socialising, and time spent alone



"Anxiety and depression were measured at approximately 18 years, using a self-administered, computerised version of the revised Clinical Interview Schedule (CIS-R) [18]
completed during a study clinic. The CIS-R asks questions about a range of symptoms and can be used to assign ICD-10 diagnoses of depression and anxiety disorders"


"In summary, our results suggest that increased computer use at age 16 is associated with an increased risk of depression and anxiety at age 18, although causality cannot be ascertained. After adjustment for potential confounders, there was little evidence of an effect of time spent texting or watching TV on risk of anxiety and depression indicating there may be a more complex relationship between screen time and mental health outcomes than simply more screen time increasing risk."






