---
title: "OpenNorth Team Launches Contribution to University of Bristol MAPS crowdsourced analysis"
author: "Corey Pembleton"
date: '2019-04-27'
slug: opennorth-maps-crowdsourced-open-science-project
categories: [ "R", "dataexploration"]
tags: ["data cleaning", "statistics", "crowdsourcing"]
keywords: ["tech", "R", "rmarkdown", "opendata"]
output:
  blogdown::html_page:
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(naniar)

```

My colleagues and I here at [OpenNorth](https://opennorth.ca) are contributing to a very interesting crowd-sourcing data analysis project out of the University of Bristol's Jean Golding Institute, *mapping the analytical paths of a crowdsourced data analysis*, or [MAPS](https://jean-golding-institute.github.io/maps/).


```{r, echo = FALSE, fig.align='center'}

blogdown::shortcode("tweet","1110577479507431425")

```

The MAPS study is largely a [Many Analysts](https://journals.sagepub.com/doi/10.1177/2515245917747646) study wherein many teams of analysts use the same data set to answer the same question seen in the tweet above. 

They've armed every analyst team with a dataset, given full access to the original research piece answering the same question, detailed instructions, and other resources on an Open Science Framework page.

The MAPS project has several research questions is seeks to answer, the primary questions are concerned with replicability and comparability between the Many Analysts:

**Primary project questions:**  
1. How do teams’ results compare to the multiverse of results?  
2. How does the variability of teams’ results compare the variability of the multiverse of results?  
3. How does each team’s results compare to all the other teams’ results?  
4. What are effective ways of visually communicating the above comparisons?  

And the secondary questions are more focused on how the experiences of the Many Analysts impacts their results:

**Secondary project questions to be answered will be**:  
5. Does domain expertise affect the results of teams’ analyses?  
6. Does statistical expertise affect the results of teams’ analyses?  
7. Are teams’ subjective beliefs about the research question associated with the results of teams’ analyses?  
8. Does the anonymisation process affect the results of teams' analyses?  

While the OpenNorth team is less concerned with the research questions of the MAPs analysis itself, and will focus on performing out replicable analysis, we will still be keeping their questions in mind to be able to contribute as meaningfully as possible.

### Approach

Generally speaking, the team has agreed to an approach which will allow us to contribute to the MAPS project and learn some new tricks along the way. We plan to break the work up in several stages: first, understanding the data and the approach used by the authors. Second, we want to explore further the iteration approach used to complete the dataset, and review how that will impact out results using a slightly different synthetic dataset. Third, we will complete our own research to better understand the ways which the variables (screen time) have been studied by other researchers, and begin developing an alternative hypothesis to test.

A challenge in volunteering to participate in a research project of this nature is being able to strike the balance between completing meaningful research, ensuring the entire team learns something new along the way, and working within a short time frame (for free). I'm excited to get started on the project, to meet the other researchers involved, to learn new statistical analysis approaches, and hopefully make some beautiful visualizations out of this interesting opportunity in shared science.




